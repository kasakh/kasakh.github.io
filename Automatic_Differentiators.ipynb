{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a short tutorial on explaining the difference between Automatic Differentiation and Symbolic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is intended for readers who have read (and not yet understood) the forward/reverse modes of Automatic Differentiation. In the notebook, there is some original experimentation, along with compiled materials from  [Automatic Differentiation in Machine Learning](https://arxiv.org/abs/1502.05767) and various other sources. \n",
    "\n",
    "It is said that AD tools are one of the most underrated and under-utilized tools in today's ML. What makes ADs so special? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is ease to see the difference between Automatic Differentiation (AD) vs Numerical Differentiation (ND)  (which is marred by approximation errors), the subtle difference between AD vs Symbolic Differentiation (SDs) is not easy to appreciate. \n",
    "\n",
    "The key idea is this: In symoblic differentiation, we first regliously evalute the complete expression and then differentiate a complex expression using sum rule, product rule, etc. However, it may so happen that we are indeed repeteadly evaluating a same expression multiple times. For e.g.\n",
    "\n",
    "$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx} \\left(f(x) + g(x)\\right) &\\leadsto \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x)\\\\\n",
    "\\frac{d}{dx} \\left(f(x)\\,g(x)\\right) &\\leadsto \\left(\\frac{d}{dx} f(x)\\right) g(x) + f(x) \\left(\\frac{d}{dx} g(x)\\right)\\; .\n",
    "\\end{aligned}\n",
    "\\label{EquationMultiplicationRule}\n",
    "\\end{equation}$\n",
    "\n",
    "\n",
    "Consider a function $h(x)=f(x)g(x)$ and the multiplication rule in the above equation. Since $h$ is a product, $h(x)$ and $\\frac{d}{dx}h(x)$ have some common components, namely $f(x)$ and $g(x)$. Note also that on the right hand side, $f(x)$ and $\\frac{d}{dx}f(x)$ appear separately. If we just proceeded to symbolically differentiate $f(x)$ and plugged its derivative into the appropriate place, we would have nested duplications of any computation that appears in common between $f(x)$ and $\\frac{d}{dx}f(x)$. Hence, careless symbolic differentiation can easily produce exponentially large symbolic expressions which take correspondingly long to evaluate. This problem is known as **expression swell**.\n",
    "\n",
    "\n",
    "When we are concerned with the accurate numerical evaluation of derivatives and not so much with their actual symbolic form, it is in principle possible to significantly simplify computations by storing only the values of intermediate sub-expressions in memory. Moreover, for further efficiency, we can interleave as much as possible the differentiation and simplification steps. This interleaving idea forms the basis of AD and provides an account of its simplest form: **apply symbolic differentiation at the elementary operation level and keep intermediate numerical results, in lockstep with the evaluation of the main function.** \n",
    "\n",
    "To illustrate the problem, consider the following example: $\\text{Iterations of the logistic map $l_{n+1}=4l_n (1-l_n)$, $l_1=x$ and the corresponding derivatives of $l_n$ with respect to $x$, illustrating expression swell.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{rr} \\hline\n",
    "n & l_n & \\frac{d}{dx}l_n & \\frac{d}{dx}l_n (Simplified form) \\\\ \\hline\n",
    "1 & x & 1 & 1 \\\\ \\hline\n",
    "2 & 4x(1 - x) & 4(1 - x) -4x & 4 - 8x \\\\ \\hline\n",
    "3 & 16x(1 - x)(1 - 2 x)^2 & 16(1 - x)(1 - 2 x)^2 - 16x(1 - 2 x)^2 - 64x(1 - x)(1 - 2 x) & 16 (1 - 10 x + 24 x^2 - 16 x^3) \\\\\n",
    "4 & 64x(1 - x)(1 - 2 x)^2 (1 - 8 x + 8 x^2)^2 & 128x(1 - x)(-8 + 16 x)(1 - 2 x)^2 (1 - 8 x + 8 x^2) + 64 (1 - x)(1 - 2 x)^2  (1 - 8 x + 8 x^2)^2 - 64x(1 - 2 x)^2 (1 - 8 x + 8 x^2)^2 - 256x(1 - x)(1 - 2 x)(1 - 8 x + 8 x^2)^2 & 64 (1 - 42 x + 504 x^2 - 2640 x^3 + 7040 x^4 - 9984 x^5 + 7168 x^6 - 2048 x^7)\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table clearly shows that the number of repetitive evaluations increase with $n$.\n",
    "\n",
    "**A rudimentary solution to counter expression swell:**\n",
    "\n",
    "For e.g.: $x(1-x)$ occurs many times in the derivative. So, while computing the $\\frac{dl}{dx}$ say at $x = 0.5$, we can compute this value of $x(1-x) = 0.5*0.5 = 0.25$ once and use it whenever we encounter. \n",
    "\n",
    "**What AD does actually**\n",
    "Rather than first getting the expression of $l_{n+1}$ entirely in terms of x and then differentiating, we can do the following: \n",
    "The derivative of $l_{n+1} = 4 l_{n} (1- l_{n})$ can be found using the chain rule $\\frac{dl_{n+1}}{dl_n}\\frac{dl_{n}}{dl_{n-1}}\\dots \\frac{dl_{1}}{dl_x}$ which simplfies to $4(1-l_{n} - l_{n})4(1-l_{n-1} - l_{n-1})\\dots4(1-x - x)$. \n",
    "\n",
    "The thing to note is that evaluating the AD way is computationally linear wrt $n$ (because we add only one $(1-l_n - l_n)$ for each increase by 1) whereas using the SD way is exponential (as evident in the table above). This linear timecomplexity is achieved due to carry-over of the derivatives at each step, rather than evaluating the derivative at the end and subsituting the value of x. This is the crux of AD. Of course, there are other important differences between AD vs SD but they are not considered here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example illustrating the difference due to AD vs SD on time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following expressions: \n",
    "$l_0 = \\frac{1}{1+e^x}$, $l_1 = \\frac{1}{1+e^{l_0}} $, ....., $l_{n} = \\frac{1}{1+e^{l_{n-1}}} $\n",
    "\n",
    "We evaluate the derivative of $l_{n}$ wrt x and compare the runtime in Mathematica (SD) vs Python (AD) for various values of $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e0335901\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\e0335901\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\e0335901\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013198895001823984\n",
      "0.0002991872100128603\n",
      "0.0005153054840075129\n",
      "0.0022935320530004903\n",
      "0.004333592132985359\n",
      "0.00917335605399603\n"
     ]
    }
   ],
   "source": [
    "## Python Code\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import time\n",
    "\n",
    "def my_sin(x,n):\n",
    "    p = x\n",
    "    y = 1 / (1+np.exp(p))\n",
    "    for i in range(n):\n",
    "        y = 1 /(1+ np.exp(y))\n",
    "        #y = (1+ np.exp(y))\n",
    "    return y\n",
    "grad_sin = grad(my_sin)\n",
    "\n",
    "#computes the mean runtime over 1000 iterations\n",
    "\n",
    "def my_timer(x):\n",
    "    start_time = time.clock()\n",
    "    times = []\n",
    "    for i in range(1000):\n",
    "        start_time = time.clock()\n",
    "        grad_sin(0.0,x)\n",
    "        times.append(time.clock() - start_time)\n",
    "    return np.mean(times)    \n",
    "\n",
    "# printing the runtime (in seconds) for values of n\n",
    "print(my_timer(1))\n",
    "print(my_timer(5))\n",
    "print(my_timer(10))\n",
    "print(my_timer(50))\n",
    "print(my_timer(100))\n",
    "print(my_timer(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the runtime here is almost linear wrt to n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent code in Mathematica: \n",
    "(for 10 iterations)\n",
    "f[x_] = Nest[1/(1 + Exp[#]) &, x, 10]\n",
    "Timing[D[f[x], x] /. x -> 0.0]\n",
    "\n",
    "$\\text{Timing}\\left[\\frac{\\partial f(x)}{\\partial x}\\text{/.}\\, x\\to 0.\\right] \\left(f(\\text{x$\\_$})=\\text{Nest}\\left[\\frac{1}{\\exp (\\text{$\\#$1})+1}\\&,x,10\\right]\\right)$\n",
    "\n",
    "The timings (in seconds) obtained are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.000000 \n",
    "0.000046875 \n",
    "0.000231662\n",
    "0.00437423\n",
    "0.15625\n",
    "1.45364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{array}{rr} \n",
    " \\text{Average} & \\text{Runtime over} & \\text{ 1000 iterations} &  \\\\ \\hline\n",
    "n & Python  & Mathematica \\\\ \\hline\n",
    "1 & 0.0013 & 0.0000 \\\\ \n",
    "5 & 0.000299 & 0.000046 \\\\ \n",
    "10 & 0.000515 & 0.000231 \\\\\n",
    "50 & 0.002935 & 0.004374 \\\\\n",
    "100 & 0.004333 & 0.15625 \\\\\n",
    "200 & 0.0091733 & 1.45364 \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the linear time complexity of Python's Autograd vs Mathematica's exponential time complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
