{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see why Policy Iteration Algorithm works. It has two steps. \n",
    "\n",
    "### Policy Evaluation Step:\n",
    "\n",
    "$v_n = r_{d_n} + \\gamma P_{d_n}v_n$ is the general vectorial form of the system of linear equations. \n",
    "\n",
    "Here, the terms $r_{d_n}, P_{d_n}$ are immediate rewards and corresponding rows of the transition matrix.\n",
    "\n",
    "\n",
    "These terms are dependent on the policy $\\Pi_n$\n",
    "\n",
    "Solving the above system of equations we can find the values of $v_n$\n",
    "\n",
    "\n",
    "### Policy Improvement Step:\n",
    "\n",
    "Assume that we were able to find a new policy $\\Pi_{n+1}$ such that \n",
    "\n",
    "\\begin{align}\n",
    "r_{d_n+1} + \\gamma P_{d_n+1}v_n & \\ge r_{d_n} + \\gamma P_{d_n}v_n \\\\\n",
    "\\implies r_{d_n+1}  & \\ge [I - \\gamma P_{d_n+1}]v_n  \\quad \\text{say this is eqn. 1}\\\\ \n",
    "\\end{align}\n",
    "\n",
    "Now, based on the new policy $\\Pi_{n+1}$,\n",
    "we can find \n",
    "$v_{n+1} = r_{d_{n+1}} + \\gamma P_{d_{n+1}}v_{n+1}$, say this is equation 2. \n",
    "\n",
    "\n",
    "\n",
    "We are going to show that $v_{n+1} \\ge v_n$ ;\n",
    "\n",
    "i.e. essentially for all the states, the newly chosen policy $\\Pi_{n+1}$ gives a better value compared to the previous policy $\\Pi_{n}$\n",
    "\n",
    "###Proof: \n",
    "\n",
    "From, equation 2, we have, \n",
    "\n",
    "$[I - \\gamma P_{d_{n+1}}]v_{n+1} = r_{d_n+1}$ \n",
    "\n",
    "From, $1 \\&2$, we have \n",
    "\n",
    "$v_{n+1} \\ge v_{n}$\n",
    "\n",
    "Essentially, the values are monotonically increasing with each iteration. \n",
    "\n",
    "This is important to understand why Policy Interation will not be stuck at a local maximum. \n",
    "\n",
    "A Policy is nothing but a state-action space.\n",
    "\n",
    "At every policy iteration step, we try to find at least one state-action which is different between  $\\Pi_{n+1}$ and $\\Pi_{n}$ and see if $ \\quad r_{d_n+1} + \\gamma P_{d_n+1}v_n  \\ge r_{d_n} + \\gamma P_{d_n}v_n$. Only if the condition is satisfied we will compute the solution to the new system of linear equations.\n",
    "\n",
    "Assume $\\Pi^*$ and $\\Pi^\\#$ are the global and local optimum respectively. \n",
    "\n",
    "Implies, $v_* \\ge v_\\#$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Assume the algorithm is stuck at the local optimum. \n",
    "\n",
    "If this is the case, then the policy improvement step will not stop at the local optimum state-action space $\\Pi^\\#$,\n",
    "as there exists at least one state-action in $\\Pi^*$ which is different from $\\Pi^\\#$  and yields a higher value of $v_{*}$ compared to $v_{\\#}$\n",
    "\n",
    "or, in other words, \n",
    "\n",
    "$[I-\\gamma P_{d_*}]v_* \\ge [I-\\gamma P_{d_*}]v_{\\#}$, assume the $[I-\\gamma P_{d_*}]$ is positive\n",
    "\n",
    "$\\implies r_{d_*} \\ge [I-\\gamma P_{d_*}]v_{\\#}$\n",
    "\n",
    "$\\implies r_{d_*} + \\gamma P_{d_*}v_{\\#} \\ge v_{\\#}$\n",
    "\n",
    "$\\implies r_{d_*} + \\gamma P_{d_*}v_{\\#} \\ge r_{d_\\#} + \\gamma P_{d_\\#}v_\\#$ \n",
    "\n",
    "Hence, the Policy iteration does not stop at a local optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
